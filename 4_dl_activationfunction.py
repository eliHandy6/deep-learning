# -*- coding: utf-8 -*-
"""4_DL_ActivationFunction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pwHoeQxcALFd7E4tY6j-ccsh3VF_KEWh

#Activation Function

###Importing Libraries
"""

from numpy import array #For Array Initialization
from numpy import random #For Randomly choosing Numbers
from numpy import dot #For Doing DOT Product
from random import choice

"""###Initialized Dataset"""

dataset = [
    (array([0,0,1]), 0), #array([x,y,b],e) x,y=Input , b=bias, e=Expected O/P to validate
    (array([0,1,1]), 1),
    (array([1,0,1]), 1),
    (array([1,1,1]), 1),
]
print(dataset)
print(array([0,0,1]), 0)

"""###Initializing Random numbers for WEIGHTS"""

weights = random.rand(3)
weights

"""###Initializing additional variables"""

r = 0.2 #learning Rate
n = 100 #Number of Iteration

"""##ACTIVATION FUNCTION

###Training & Evaluating using STEP Activation Function
"""

activationFn = lambda x: 0 if x < 0 else 1 #step activation function (if i/p is negative o/p is 0 else 1)
for j in range(n):
    x, expected = choice(dataset)
    result = dot(weights, x)
    err = expected-activationFn(result)
    weights += r * err * x
print("Training & Evaluating using STEP Activation Function")
print()
for x, _ in dataset:
    result = dot(x, weights)
    print("ResultBAFn: {} ResultAFn: {}".format(round(result,3), activationFn(result)))
print()
"""###Training & Evaluating using LINEAR Activation Function

"""

activationFn = lambda x: x

for j in range(n):
    x, expected = choice(dataset)
    result = dot(weights, x)
    err = expected-activationFn(result)
    weights += r * err * x

print("Training & Evaluating using LINEAR Activation Function")

for x, _ in dataset:
    result = dot(x, weights)
    print("ResultBAFn: {} ResultAFn: {}".format(round(result,3), activationFn(result)))
print()
"""###Training & Evaluating using SIGMOID Activation Function"""

import numpy as np
activationFn = lambda x: 1/(1+np.exp(-x))
error = []
for j in range(n):
    x, expected = choice(dataset)
    result = dot(weights, x)
    err = expected-activationFn(result)
    error.append(err)
    weights += r * err * x

print("Training & Evaluating using SIGMOID Activation Function")

for x, _ in dataset:
    result = dot(x, weights)
    print("ResultBAFn: {} ResultAFn: {}".format(round(result,3), activationFn(result)))
print()
"""###Training & Evaluating using RELU Activation Function"""

activationFn = lambda x: 0 if x < 0 else x
for j in range(n):
    x, expected = choice(dataset)
    result = dot(weights, x)
    err = expected-activationFn(result)
    weights += r * err * x

print("Training & Evaluating using RELU Activation Function")


for x, _ in dataset:
    result = dot(x, weights)
    print("ResultBAFn: {} ResultAFn: {}".format(round(result,3), activationFn(result)))
print()







"""###Training & Evaluating using SOFTMAX Activation Function"""

activationFn = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)
error = []
for j in range(n):
    x, expected = choice(dataset)
    result = dot(weights, x)
    err = expected-activationFn(result)
    error.append(err)
    weights += r * err * x

print("Training & Evaluating using SOFTMAX Activation Function")

for x, _ in dataset:
    result = dot(x, weights)
    print("ResultBAFn: {} ResultAFn: {}".format(round(result,3), activationFn(result)))
print()